---
title: "Lab 6: Logistic Regression and Support Vector Machines"
author: "Kaori Hirano"
date: "6/12/23"
format: pdf
---
# NEED TO CHECK PLOT AND SCALE IS TRUE OR FALSE
# Packages

```{r load-packages}
# load packages
suppressPackageStartupMessages(library(tidyverse))
library(broom) # for tidy function
library(patchwork) # for plot placement
library(ggplot2)
suppressPackageStartupMessages(library(openintro))
suppressPackageStartupMessages(library(boot))
suppressPackageStartupMessages(library(ROCR))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(e1071))
suppressPackageStartupMessages(library(glmnet))
```

# Data  

```{r load-data}
d <- email
```


# Exercises 

## Data Visualization

### Q1

One variable I think is likely an indicator of spam is the word "winner." This is something that shows up in a lot of spam emails because it is a common way of scamming people. Winner also does not often show up in non-spam regular emails, so the presence of winner would likely indicate that the email is spam. 

Another variable that would help indicate if spam or not is inherit. Inherit is not something that occurs in most emails, but is something that scam email would probably include. The more times inherit is mentioned, I would expect that the email is more likely to be spam.  

### Q2

```{r data-viz}
ggplot(d, aes(x = inherit, y = winner, colour = spam)) +
  geom_point() +
  geom_jitter() +
  labs(title = "Spam prediction by inheritance mentions and winner presence", x = 'inheritance mentions', y = 'winner presence')
```
The relationships do not appear to match what I expected to see. There are a lot more instances of not spam emails including the word winner than spam emails, which I did not expect. Similarly, there were many inheritance mentions in spam and nonspam emails, with even more in nonspam emails. 

The classes do not seem linearly separable because of the overlap they share. 

## Two-Variable Models

### Q3
```{r split-data}
# splits data in training and test set by 70/30
set.seed(145)
train <- sample(c(TRUE, FALSE), nrow(d),
     replace = TRUE, prob=c(.7,.3))
test <- (!train)
```

### Q4
Fit a logistic regression model with the two variables you chose above. Plot the ROC curve and
report the AUC for the test data (you can use ROCR, pROC, or ggplot2 for the visualization).
Based on the coefficients, do the relationships match the ones you expected to see (which you
wrote down in Q1)
```{r log-reg}
# creates glm and prints summary tab
glm_fits <- glm(spam ~ inherit + winner, d, family = binomial)
summary(glm_fits)
```
```{r roc}
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf, ...)
}

# setting up svm
svmfit_optl <- svm(spam ~ inherit + winner, data = d[train, ], 
    kernel = "radial", gamma = 2, cost = 1, 
    decision.values = T)
fittedl <- attributes(
    predict(svmfit_optl, d[train, ],
            decision.values = TRUE)
  )$decision.values

# svm with higher gamma
rocplot(-fitted, d[train, "spam"], main = "Training Data")
svmfit_flexl <- svm(spam ~ inherit + winner, data = d[train, ], 
    kernel = "radial", gamma = 50, cost = 1, 
    decision.values = T)
fitted_flexl <- attributes(
    predict(svmfit_flexl, d[train, ], decision.values = T)
  )$decision.values
rocplot(-fitted_flexl, d[train, "spam"], add = T,
        col = "red")

# test
fitted_testl <- attributes(
    predict(svmfit_optl, d[-train, ], decision.values = T)
  )$decision.values
rocplot(-fitted_testl, d[-train, "spam"], main = "Test Data")

fitted_flex_testl <- attributes(
    predict(svmfit_flexl, d[-train, ],
            decision.values = T)
  )$decision.values
rocplot(-fitted_flex_testl, d[-train, "spam"], add = T,
        col = "red")

pred_optl <- prediction(-fitted_testl, d[-train, "spam"])
auc_optl <- performance(pred_optl, "auc")

pred_flexl <- prediction(-fitted_flex_testl, d[-train, "spam"])
auc_flexl <- performance(pred_flexl, "auc")

auc_flexl@y.values
auc_optl@y.values # this one is better!
```
Based on the coefficients, I do see the relationship I expected to see in q1. Having winner in an email makes it more likely to be associated withbeing spam, same with having inherit/inheritance to a lesser degree, as seen by the coefficients of 1.51 and .20. 

The AUC is .5179

### Q5
Fit a support vector machine with the two variables you chose above.
Use cross-validation to find the optimal model. Set the seed to 246 before doing so and the
search over the following tuning parameter space:
What is the optimal combination of parameters
Plot the decision boundary using the built in plot() function.
Plot the ROC curve and report the AUC for the test data (you can use ROCR, pROC, or ggplot2
for the visualization).

```{r tuning-svm}
# tune didn't like having both radial and linear as arguments
# so I just did two tests, one for linear and one for radial
list(
  cost = c(0.1, 1, 10),
  gamma = c(0.5, 1, 2, 3),
  kernel = c("linear", "radial")
    )
```

```{r svm}
# data is already in the proper format with y as factor

# cross validation for tuning parameters
set.seed(246)
tune_outr <- tune(svm, spam ~ inherit + winner, data = d, kernel = c("radial"), 
    ranges = list(gamma = c(.5, 1, 2, 3), cost = c(0.1,  1.0, 10.0)))
summary(tune_outr)

tune_outl <- tune(svm, spam ~ inherit + winner, data = d, kernel = c("linear"), 
    ranges = list(gamma = c(.5, 1, 2, 3), cost = c(0.1,  1.0, 10.0)))
summary(tune_outl)

# uses optimal to make decision boundary
svmbest <- svm(spam ~ inherit + winner, data = d, kernel = "linear", 
    cost = .1, gamma = .5, scale = TRUE)
```

The optimal tuning parameters are for radial gamma .5 and cost .1 with .0935 performance, 
and with linear gamma .5 and cost .1 with performance of .0936, making the linear
gamma .5 and cost .1 the optimal parameter combination. 

```{r svc-plot}
# NEED TO GET THE PLOT TO WORK
# making data so only the variables in svc are in the data frame
datasvc <- d[c('spam', 'inherit', 'winner')]
# plotting decision boundary with built in plot
plot(svmbest, datasvc) 
```

```{r roc-plot}
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf, ...)
}

# uses svmbest from prior question as model
fitteds <- attributes(
    predict(svmbest, d[train, ],
            decision.values = TRUE)
  )$decision.values

# test data is used for predictions
fitted_tests <- attributes(
    predict(svmbest, d[-train, ], decision.values = T)
  )$decision.values

# plots ROC
rocplot(-fitted_tests, d[-train, "spam"], main = "Test Data")

# gets predictions so AUC can be calculated
pred_opts <- prediction(-fitted_tests, d[-train, "spam"])
auc_opts <- performance(pred_opts, "auc")

# prints AUC
auc_opts@y.values

```
The AUC is .4756 for the model with the optimal parameters above. 

## Full Models

### Q6
Repeat Q4, but this time use all variables in emails.
Compare performance on the test set with the two-variable model

```{r log-reg-full}
# creates glm and prints summary tab
glm_fit_full <- glm(spam ~ to_multiple + from + cc + sent_email + time + image + attach + dollar + winner + inherit + viagra + password + num_char + line_breaks + format + re_subj + exclaim_subj + urgent_subj + exclaim_mess + number, d, family = binomial)
summary(glm_fit_full)
```

```{r roc}
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf, ...)
}

# setting up svm
svmfit_lfull <- svm(spam ~ to_multiple + from + cc + sent_email + time + image + attach + dollar + winner + inherit + viagra + password + num_char + line_breaks + format + re_subj + exclaim_subj + urgent_subj + exclaim_mess + number, data = d[train, ], 
    kernel = "radial", gamma = 2, cost = 1, 
    decision.values = T)
fittedlf <- attributes(
    predict(svmfit_lfull, d[train, ],
            decision.values = TRUE)
  )$decision.values

# svm with higher gamma
rocplot(-fittedlf, d[train, "spam"], main = "Training Data")
svmfit_flexlf <- svm(spam ~ to_multiple + from + cc + sent_email + time + image + attach + dollar + winner + inherit + viagra + password + num_char + line_breaks + format + re_subj + exclaim_subj + urgent_subj + exclaim_mess + number, data = d[train, ], 
    kernel = "radial", gamma = 50, cost = 1, 
    decision.values = T)
fitted_flexlf <- attributes(
    predict(svmfit_flexlf, d[train, ], decision.values = T)
  )$decision.values
rocplot(-fitted_flexlf, d[train, "spam"], add = T,
        col = "red")

# test
fitted_testlf <- attributes(
    predict(svmfit_lfull, d[-train, ], decision.values = T)
  )$decision.values
rocplot(-fitted_testlf, d[-train, "spam"], main = "Test Data")
fitted_flex_testlf <- attributes(
    predict(svmfit_flexlf, d[-train, ],
            decision.values = T)
  )$decision.values
rocplot(-fitted_flex_testlf, d[-train, "spam"], add = T,
        col = "red")

pred_optlf <- prediction(-fitted_testlf, d[-train, "spam"])
auc_optlf <- performance(pred_optlf, "auc")

pred_flexlf <- prediction(-fitted_flex_testlf, d[-train, "spam"])
auc_flexlf <- performance(pred_flexlf, "auc")

auc_flexlf@y.values # this one is better!
auc_optlf@y.values
```
The AUC is .9617 for the flexible model and .9047 for the less flexible model, so the flexible model is better in this case. 

The coefficients here largely match what I would expect. I thought it was interesting that image and attachments were associated with spam, but it makes sense when thinking about how many common emails actually don't include either of these. However, words like password, inherit, and winner, all make sense as spam and I'm not surprised to see them. Overall, these coeffifients were not that surprising. 

The AUC here is much higher than the two variable model. 

```{r compares-logistic}
tprfpr_optl <- performance(pred_optl, "tpr", "fpr") 

# get tpr and fpr for flex model
tprfpr_flexlf <- performance(pred_flexlf, "tpr", "fpr") 

#below extracts the y values and x values we need for gamma = 2 model
tpr_optl <- unlist(tprfpr_optl@y.values) 
fpr_optl <- unlist(tprfpr_optl@x.values)

# do the same for 2 model
tpr_flexlf <- unlist(tprfpr_flexlf@y.values) 
fpr_flexlf <- unlist(tprfpr_flexlf@x.values)

# combine into dataframe
roc_data_testlf <- bind_rows(
  data.frame(tpr = tpr_optl,
             fpr = fpr_optl,
             Model = "Two Variables"),
  data.frame(tpr = tpr_flexlf,
             fpr = fpr_flexlf,
             Model = "Full Variables")
)

# plot using ggplot
ggplot(roc_data_testlf, aes(x = fpr, y = tpr, color = Model)) +
  geom_line() +
  xlim(c(0, 1)) +
  ylim(c(0, 1)) +
  labs(x = "False Positive Rate",
        y = "True Positive Rate",
        title = "Comparing ROC Curves for Two Different Models") +
  theme_classic()
```
This visualization makes it clear that the full variable model is a much better fit for the data than the two variable model, as seen by the significantly higher AUC and the model  being very close to the top left corner. 

### Q7
Repeat Q5 (you do not have to plot the decision boundary) with all the variables in email
except for viagra.
2 This time set the seed to 247 before cross-validation. To save on time,
do only 5-fold cross-validation (read the helpfile for tune() to figure out which argument you
need to use to do this).
Compare performance on the test set with the two-variable model.

```{r svm-full}
# data is already in the proper format with y as factor

# cross validation for tuning parameters
set.seed(247)
tune_outr <- tune(svm, spam ~ to_multiple + from + cc + sent_email + time + image + attach + dollar + winner + inherit + password + num_char + line_breaks + format + re_subj + exclaim_subj + urgent_subj + exclaim_mess + number, data = d, kernel = c("radial"), 
    ranges = list(gamma = c(.5, 1, 2, 3), cost = c(0.1,  1.0, 10.0)), tunecontrol = tune.control(cross=5))
summary(tune_outr)

set.seed(247)
tune_outl <- tune(svm, spam ~ to_multiple + from + cc + sent_email + time + image + attach + dollar + winner + inherit + password + num_char + line_breaks + format + re_subj + exclaim_subj + urgent_subj + exclaim_mess + number, data = d, kernel = c("linear"), 
    ranges = list(gamma = c(.5, 1, 2, 3), cost = c(0.1,  1.0, 10.0)), tunecontrol = tune.control(cross=5))
summary(tune_outl)

# uses optimal parameters to make svm
svmfull <- svm(spam ~ to_multiple + from + cc + sent_email + time + image + attach + dollar + winner + inherit + password + num_char + line_breaks + format + re_subj + exclaim_subj + urgent_subj + exclaim_mess + number, data = d, kernel = "linear", 
    cost = 10, gamma = .5, scale = TRUE)
svmfull
```
The optimal tuning parameters are for radial gamma 2 and cost 10 with .069 performance and error, 
and with linear gamma .5 and cost 10 with performance and error of .092, making the linear
gamma .5 and cost 10 the optimal parameter combination. 

```{r roc-plot-full}
# plotting full svm with optimal parameters
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf, ...)
}

# uses svmfull from prior question as model
fittedsf <- attributes(
    predict(svmfull, d[train, ],
            decision.values = TRUE)
  )$decision.values

# test data is used for predictions
fitted_testsf <- attributes(
    predict(svmfull, d[-train, ], decision.values = T)
  )$decision.values

# plots ROC
rocplot(-fitted_testsf, d[-train, "spam"], main = "Test Data")

# gets predictions so AUC can be calculated
pred_optsf <- prediction(-fitted_testsf, d[-train, "spam"])
auc_optsf <- performance(pred_optsf, "auc")

# prints AUC
auc_optsf@y.values

```
The AUC for the full model with the optimal parameters is .812. 

```{r compare-svm-models}
# full model svm information
tprfpr_optsf <- performance(pred_optsf, "tpr", "fpr") 

# 2 var svm
tprfpr_opts <- performance(pred_opts, "tpr", "fpr") 

#below extracts the y values and x values we need for gamma = 2 model
tpr_optsf <- unlist(tprfpr_optsf@y.values) 
fpr_optsf <- unlist(tprfpr_optsf@x.values)
# do the same for gamma = 50 model
tpr_opts <- unlist(tprfpr_opts@y.values) 
fpr_opts <- unlist(tprfpr_opts@x.values)

# combine into dataframe
roc_data_tests <- bind_rows(
  data.frame(tpr = tpr_optsf,
             fpr = fpr_optsf,
             Model = "Full Variables (minus viagra)",
  data.frame(tpr = tpr_opts,
             fpr = fpr_opts,
             Model = "2 Variables")
)


# plot using ggplot
ggplot(roc_data_tests, aes(x = fpr, y = tpr, color = Model)) +
  geom_line() +
  xlim(c(0, 1)) +
  ylim(c(0, 1)) +
  labs(x = "False Positive Rate",
        y = "True Positive Rate",
        title = "Comparing ROC Curves for Two Different Models") +
  theme_classic()
```
This visualization illustrates that the full variable model is a better predictor of the data than the 2 variables because there is a much higher AUC for the full vs 2 variable model. 

### Q8 
```{r compares-full}
# the data have already been converted into the forms we need
# just need to bind together and plot

# combine into dataframe
roc_data_testf <- bind_rows(
  data.frame(tpr = tpr_optsf,
             fpr = fpr_optsf,
             Model = "Full Variables SVM (minus viagra)"),
  data.frame(tpr = tpr_flexlf,
             fpr = fpr_flexlf,
             Model = "Full Variables Logistic")
)


# plot using ggplot
ggplot(roc_data_testf, aes(x = fpr, y = tpr, color = Model)) +
  geom_line() +
  xlim(c(0, 1)) +
  ylim(c(0, 1)) +
  labs(x = "False Positive Rate",
        y = "True Positive Rate",
        title = "Comparing ROC Curves for Two Different Models") +
  theme_classic()

```
This visualization makes it clear that there is a much higher AUC in the logistic model than in the svm model. 

